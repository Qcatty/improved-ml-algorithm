{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d499751",
   "metadata": {},
   "source": [
    "# Predicting ground states for 2D Heisenberg models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de9d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic functionalities\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import ast\n",
    "import datetime as dt\n",
    "from timeit import default_timer as timer\n",
    "from os import path\n",
    "\n",
    "# Neural tangent kernel\n",
    "import jax\n",
    "from neural_tangents import stax\n",
    "\n",
    "# Traditional ML methods and techniques\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d17dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "length = 4 # length = 4, 5, 6, 7, 8, 9 for orig; only 4, 5, 6, 7 for new\n",
    "width = 5\n",
    "\n",
    "Xfull = [] # Shape = (number of data) x (number of params)\n",
    "Ytrain = [] # Shape = (number of data) x (number of pairs), estimated 2-point correlation functions\n",
    "Yfull = [] # Shape = (number of data) x (number of pairs), exact 2-point correlation functions\n",
    "\n",
    "def get_path_prefix(data='orig'):\n",
    "    prefix = './heisenberg_data/heisenberg_{}x{}'.format(length, width)\n",
    "    if data == 'new':\n",
    "        prefix = './new_data/data_{}x{}/simulation_{}x{}'.format(length, width, length, width)\n",
    "    return prefix\n",
    "    \n",
    "data_name = 'new'\n",
    "prefix = get_path_prefix(data=data_name)\n",
    "\n",
    "for idx in range(1, 301):\n",
    "    if path.exists('{}_id{}_XX.txt'.format(prefix, idx)) == False:\n",
    "        continue\n",
    "    with open('{}_id{}_samples.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        classical_shadow = [[int(c) for i, c in enumerate(line.split(\"\\t\"))] for line in f]\n",
    "        for i in range(length * 5):\n",
    "            for j in range(length * 5):\n",
    "                if i == j:\n",
    "                    single_data.append(1.0)\n",
    "                    continue\n",
    "                corr = 0\n",
    "                cnt = 0\n",
    "                for shadow in classical_shadow:\n",
    "                    if shadow[i] // 2 == shadow[j] // 2:\n",
    "                        corr += 3 if shadow[i] % 2 == shadow[j] % 2 else -3\n",
    "                    cnt += 1\n",
    "                single_data.append(corr / cnt)\n",
    "        Ytrain.append(single_data)\n",
    "    with open('{}_id{}_XX.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        for line in f:\n",
    "            for i, c in enumerate(line.split(\"\\t\")):\n",
    "                v = float(c)\n",
    "                single_data.append(v)\n",
    "        Yfull.append(single_data)\n",
    "    with open('{}_id{}_couplings.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        for line in f:\n",
    "            for i, c in enumerate(line.split(\"\\t\")):\n",
    "                v = float(c)\n",
    "                single_data.append(v)\n",
    "        Xfull.append(single_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee93cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data (N) * number of params (m) = (300, 31)\n",
      "number of data (N) * number of pairs = (300, 400)\n"
     ]
    }
   ],
   "source": [
    "# Print information\n",
    "\n",
    "Xfull = np.array(Xfull)\n",
    "print(\"number of data (N) * number of params (m) =\", Xfull.shape)\n",
    "Ytrain = np.array(Ytrain)\n",
    "Yfull = np.array(Yfull)\n",
    "print(\"number of data (N) * number of pairs =\", Yfull.shape)\n",
    "\n",
    "# print(Xfull[0])\n",
    "# print(Yfull[0].reshape((length * width, length * width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb26787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05412756  0.78486598  0.67886148 -0.8011389  -0.76692495  0.04528856\n",
      " -0.65615133 -0.51053397  0.16776154  0.23954635  0.11006126  0.08825837\n",
      " -0.45139873  0.01494092 -0.04309056  0.48323862  0.76089512 -0.89362265\n",
      "  0.72244808 -0.11372986 -0.85407612 -0.62173286 -0.16480284  0.11637334\n",
      " -0.65395904  0.74126967 -0.02007992 -0.66629825 -0.20805749 -0.62617995\n",
      "  0.26462721]\n"
     ]
    }
   ],
   "source": [
    "# Normalize Xfull\n",
    "\n",
    "xmin = np.amin(Xfull)\n",
    "xmax = np.amax(Xfull)\n",
    "\n",
    "# normalize so that all entries are between -1 and 1 using min-max feature scaling\n",
    "Xfull_norm = np.array(list(map(lambda row : list(map(lambda x : -1 + 2*(x - xmin)/(xmax - xmin), row)), Xfull)))\n",
    "\n",
    "print(Xfull_norm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfbf14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 6), (1, 2), (2, 7), (2, 3), (3, 8), (3, 4), (4, 9), (4, 5), (5, 10), (6, 11), (6, 7), (7, 12), (7, 8), (8, 13), (8, 9), (9, 14), (9, 10), (10, 15), (11, 16), (11, 12), (12, 17), (12, 13), (13, 18), (13, 14), (14, 19), (14, 15), (15, 20), (16, 17), (17, 18), (18, 19), (19, 20)]\n"
     ]
    }
   ],
   "source": [
    "# Categorizing pairs of qubits by distance\n",
    "\n",
    "# grid of qubits\n",
    "grid = np.array(range(1, length * width + 1)).reshape((length, width))\n",
    "\n",
    "# generate all edges in grid in same order as Xfull\n",
    "all_edges = []\n",
    "for i in range(0, length):\n",
    "    for j in range(1, width + 1):\n",
    "        if i != length - 1:\n",
    "            all_edges.append((width * i + j, width * (i + 1) + j))\n",
    "        if j != width:\n",
    "            all_edges.append((width * i + j, width * i + j + 1))\n",
    "print(all_edges)\n",
    "            \n",
    "def calc_distance(q1, q2):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid\n",
    "    # Output l1 distance between q1 and q2 in grid\n",
    "\n",
    "    pos1 = np.array(np.where(grid == q1)).T[0]\n",
    "    pos2 = np.array(np.where(grid == q2)).T[0]\n",
    "\n",
    "    return np.abs(pos1[0] - pos2[0]) + np.abs(pos1[1] - pos2[1])\n",
    "\n",
    "def get_nearby_qubit_pairs(d):\n",
    "    # Given distance d > 0\n",
    "    # Output all pairs of qubits that are within distance d of each other\n",
    "    \n",
    "    if d == 1:\n",
    "        return all_edges\n",
    "    \n",
    "    qubit_pairs = []\n",
    "    for q1 in range(1, length * width + 1):\n",
    "        for q2 in range(1, length * width + 1):\n",
    "            dist = calc_distance(q1, q2)\n",
    "            pair = tuple(sorted((q1, q2)))\n",
    "            if dist == d and pair not in qubit_pairs:\n",
    "                qubit_pairs.append(pair)\n",
    "    \n",
    "    return qubit_pairs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9441798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding local patches of a given radius\n",
    "\n",
    "def get_local_region_qubits(q, delta1):\n",
    "    # Given a qubit q (1-indexed integer) in length x width grid and radius delta1\n",
    "    # delta1 = -1 if all qubits are in local region\n",
    "    # Output list of qubits (1-indexed integers) within a radius of delta1 of q\n",
    "    \n",
    "    if delta1 == 0:\n",
    "        return [q]\n",
    "    elif delta1 == -1:\n",
    "        return list(range(1, length * width + 1))\n",
    "    \n",
    "    local_qubits = []\n",
    "    for q2 in range(1, length * width + 1):\n",
    "        dist = calc_distance(q, q2)\n",
    "        \n",
    "        if dist <= delta1:\n",
    "            local_qubits.append(q2)\n",
    "    \n",
    "    return local_qubits\n",
    "\n",
    "def get_local_region_edges(q1, q2, delta1):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid and radius delta1\n",
    "    # delta1 = -1 if all qubits are in local region\n",
    "    # Output list of tuples of qubits (1-indexed integers) corresponding to edges in local region of radius delta1\n",
    "\n",
    "    if delta1 == 0:\n",
    "        return [(q1, q2)]\n",
    "    elif delta1 == -1:\n",
    "        return all_edges\n",
    "\n",
    "    local_qubits = list(set(get_local_region_qubits(q1, delta1) + get_local_region_qubits(q2, delta1)))\n",
    "    \n",
    "    local_edges = []\n",
    "    for edge in all_edges:\n",
    "        (q1, q2) = edge\n",
    "        if q1 in local_qubits and q2 in local_qubits:\n",
    "            local_edges.append(edge)\n",
    "\n",
    "    return local_edges\n",
    "\n",
    "def get_local_region_params(q1, q2, delta1, data, i):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid, radius delta1, and input data (i.e., Xfull)\n",
    "    # delta1 = -1 if all qubits are considered nearby\n",
    "    # Output data but only for parameters corresponding to edges within radius delta1\n",
    "    \n",
    "    edges = get_local_region_edges(q1, q2, delta1)\n",
    "    \n",
    "    indices = [all_edges.index(edge) for edge in edges]\n",
    "    \n",
    "    return np.array([data[i][j] for j in sorted(indices)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c47e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges: [(1, 6), (1, 2), (2, 7), (2, 3), (6, 7)]\n",
      "params: [ 0.05412756  0.78486598  0.67886148 -0.8011389   0.11006126]\n"
     ]
    }
   ],
   "source": [
    "print('edges: ' + str(get_local_region_edges(1,2,1)))\n",
    "print('params: ' + str(get_local_region_params(1,2,1, Xfull_norm, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5465a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature mapping\n",
    "\n",
    "def get_feature_vectors(delta1, R, data, omega, gamma=1.0, q1=0, q2=1):\n",
    "    # Given radius delta1 and hyperparameter R (number of nonlinear features per local region), input data, and fixed randomness omega\n",
    "    # delta1 = -1 if all qubits are considered nearby\n",
    "    # Output concatenated feature vectors\n",
    "    \n",
    "    # to store all concatenated feature vectors\n",
    "    all_feature_vectors = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        feature_vector_concat = [0]\n",
    "        # iterate over all possible local regions\n",
    "        n = len(all_edges)\n",
    "#         for k in range(n):\n",
    "#             (q1, q2) = all_edges[k]\n",
    "        data_local = get_local_region_params(q1, q2, delta1, data, i)\n",
    "        m_local = len(data_local)\n",
    "\n",
    "        # do nonlinear feature map on each vector in data_local\n",
    "        feature_vector = []\n",
    "\n",
    "        for j in range(R):\n",
    "            omega_j = omega[0][j]\n",
    "            val = np.exp(np.dot(omega_j, data_local) * gamma / (m_local ** 0.5) * 1j)\n",
    "            feature_vector.append(np.real(val))\n",
    "            feature_vector.append(np.imag(val))\n",
    "\n",
    "        # concatenate feature vectors together\n",
    "        feature_vector_concat += feature_vector\n",
    "            \n",
    "        all_feature_vectors.append(feature_vector_concat)\n",
    "        \n",
    "    # note all_feature_vectors is of size number of data (N) x (2 * R * number of local regions)\n",
    "    return np.array(all_feature_vectors)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff0fc07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "0 0.4 0.0001220703125 0.24416895871292196 0.21646611389022144\n",
      "5 0.4 0.0001220703125 0.1789006530946255 0.1618792340978844\n",
      "5 0.5 0.0001220703125 0.17878688190303293 0.1619598341813617\n",
      "5 0.6 0.0001220703125 0.17866876642420854 0.16207662085355692\n",
      "5 0.65 0.000244140625 0.17861077379194304 0.1621082341346369\n",
      "5 0.7 0.000244140625 0.1785547497352205 0.1621937869152467\n",
      "5 0.75 0.00048828125 0.17851036299127407 0.1622159349343749\n",
      "(0.17851036299127407, 0.1622159349343749)\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = 0\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=0.4, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "with open('./results/{}_data/results_{}x{}_{}_data_lasso_R=0-320_C=-13--9_gamma=0.4-1.0_delta1={}_local_feature_qubits{}.txt'.format(data_name, length, width, data_name, delta1, d), 'w') as f:\n",
    "    for (q1, q2) in qubits[7:8]:\n",
    "        print('(q1, q2) =', (q1, q2))\n",
    "        print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "        def train_and_predict():\n",
    "            # consider the pair (q1, q2)\n",
    "            global q1, q2\n",
    "\n",
    "            # training data (estimated from measurement data)\n",
    "            y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=0.4, random_state=0)\n",
    "\n",
    "            # testing data (exact expectation values)\n",
    "            y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=0.4, random_state=0)\n",
    "\n",
    "            # use cross validation to find the best hyperparameters\n",
    "            best_cv_score, test_score = 999.0, 999.0\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=100000)\n",
    "            # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "            \n",
    "            for R in [0, 5, 10, 20, 40, 80, 160, 320]:\n",
    "                #for gamma in [0.0625, 0.125, 0.25, 0.5, 1.0, 2.0]:\n",
    "                #for gamma in [0.125, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 1.0]:\n",
    "                for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                    # feature mapping\n",
    "                    Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                    Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                    for C in [2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9)]:\n",
    "                        score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                        if best_cv_score > score:\n",
    "                            clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                            test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                            best_cv_score = score\n",
    "#                             print(clf.coef_)\n",
    "                            print(R, gamma, C, score, test_score)\n",
    "                            print(R, gamma, C, score, test_score, file=f)\n",
    "\n",
    "            return best_cv_score, test_score\n",
    "\n",
    "        print(train_and_predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1276aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "0 0.4 0.0001220703125 0.24416895871292196 0.21646611389022144\n",
      "5 0.4 0.0001220703125 0.10813323730378203 0.07281869844627938\n",
      "5 0.5 0.0001220703125 0.10804954786940828 0.07307769007835253\n",
      "10 0.4 0.0001220703125 0.10477482366280395 0.07108512999576383\n",
      "10 0.5 0.0001220703125 0.10399328571241404 0.07128614251026795\n",
      "10 0.6 0.0001220703125 0.10372407388868332 0.07150238532470404\n",
      "10 0.65 0.0001220703125 0.10362438524536566 0.07166208327010802\n",
      "10 0.65 0.000244140625 0.10358927983128174 0.07114951304493537\n",
      "10 0.7 0.0001220703125 0.10358292264703743 0.07156248502491695\n",
      "10 0.7 0.000244140625 0.10334419804023778 0.07132937183182318\n",
      "10 0.75 0.000244140625 0.10322142139996074 0.07155772663702444\n",
      "10 1.0 0.0001220703125 0.1030064061442254 0.07360883967585354\n",
      "10 1.0 0.000244140625 0.10250269370515128 0.0724309603306534\n",
      "10 1.0 0.00048828125 0.10231720537070732 0.07242828255706024\n",
      "20 0.5 0.0001220703125 0.10198522434415755 0.06846423819569827\n",
      "20 0.6 0.0001220703125 0.1014938968572281 0.0691299262065676\n",
      "20 0.7 0.000244140625 0.10141851167521812 0.06864280806065473\n",
      "20 0.75 0.000244140625 0.10134188004834606 0.06879151957883789\n",
      "20 1.0 0.00048828125 0.1012989834019585 0.06831606730733729\n",
      "40 0.4 0.0001220703125 0.100884236957716 0.07273287281129989\n",
      "40 0.5 0.000244140625 0.10086487499948564 0.0721581352833211\n",
      "40 0.6 0.000244140625 0.10079632138630223 0.0721638071561911\n",
      "40 0.65 0.000244140625 0.1007741503813628 0.07211045692008256\n",
      "40 0.7 0.00048828125 0.1005823693073578 0.07207337107674987\n",
      "40 0.75 0.00048828125 0.10038887907660214 0.07205614481883556\n",
      "160 0.75 0.00048828125 0.1003816351503762 0.07182106772403589\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = 1\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=0.4, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "with open('./results/{}_data/results_{}x{}_{}_data_lasso_R=0-320_C=-13--9_gamma=0.4-1.0_delta1={}_local_feature_qubits{}.txt'.format(data_name, length, width, data_name, delta1, d), 'w') as f:\n",
    "    for (q1, q2) in qubits[7:8]:\n",
    "        print('(q1, q2) =', (q1, q2))\n",
    "        print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "        def train_and_predict():\n",
    "            # consider the pair (q1, q2)\n",
    "            global q1, q2\n",
    "\n",
    "            # training data (estimated from measurement data)\n",
    "            y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=0.4, random_state=0)\n",
    "\n",
    "            # testing data (exact expectation values)\n",
    "            y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=0.4, random_state=0)\n",
    "\n",
    "            # use cross validation to find the best hyperparameters\n",
    "            best_cv_score, test_score = 999.0, 999.0\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=100000)\n",
    "            # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "            \n",
    "            for R in [0, 5, 10, 20, 40, 80, 160, 320]:\n",
    "                #for gamma in [0.0625, 0.125, 0.25, 0.5, 1.0, 2.0]:\n",
    "                #for gamma in [0.125, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 1.0]:\n",
    "                for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                    # feature mapping\n",
    "                    Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                    Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                    for C in [2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9)]:\n",
    "                        score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                        if best_cv_score > score:\n",
    "                            clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                            test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                            best_cv_score = score\n",
    "                            print(R, gamma, C, score, test_score)\n",
    "                            print(R, gamma, C, score, test_score, file=f)\n",
    "\n",
    "            return best_cv_score, test_score\n",
    "\n",
    "        print(train_and_predict(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ef50a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "0 0.4 0.0001220703125 0.24416895871292196 0.21646611389022144\n",
      "5 0.4 0.0001220703125 0.20229873740843382 0.1817122697299311\n",
      "5 0.4 0.000244140625 0.20137253929321758 0.18063684948168757\n",
      "5 0.4 0.00048828125 0.2009687967042823 0.17905824831188163\n",
      "10 0.4 0.0001220703125 0.12299014048510815 0.10092773710778855\n",
      "20 0.4 0.0001220703125 0.10007078374495208 0.07201742614231753\n",
      "20 0.4 0.000244140625 0.09904998414120891 0.07323555547826155\n",
      "20 0.4 0.00048828125 0.09874304086038602 0.07393942676791214\n",
      "80 0.4 0.0001220703125 0.09475774991572114 0.0774846650497278\n",
      "80 0.4 0.000244140625 0.09325825650232437 0.07587750930337793\n",
      "160 0.4 0.000244140625 0.09239314244221877 0.07388705028238418\n",
      "320 0.4 0.000244140625 0.09226527478673943 0.06955024566367266\n",
      "320 0.6 0.00048828125 0.09211184163936173 0.06861661630425533\n",
      "320 1.0 0.0009765625 0.09163093720612146 0.06801516046714821\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = 2\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=0.4, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "with open('./results/{}_data/results_{}x{}_{}_data_lasso_R=0-320_C=-13--9_gamma=0.4-1.0_delta1={}_local_feature_qubits{}.txt'.format(data_name, length, width, data_name, delta1, d), 'w') as f:\n",
    "    for (q1, q2) in qubits[7:8]:\n",
    "        print('(q1, q2) =', (q1, q2))\n",
    "        print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "        def train_and_predict():\n",
    "            # consider the pair (q1, q2)\n",
    "            global q1, q2\n",
    "\n",
    "            # training data (estimated from measurement data)\n",
    "            y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=0.4, random_state=0)\n",
    "\n",
    "            # testing data (exact expectation values)\n",
    "            y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=0.4, random_state=0)\n",
    "\n",
    "            # use cross validation to find the best hyperparameters\n",
    "            best_cv_score, test_score = 999.0, 999.0\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=100000)\n",
    "            # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "            \n",
    "            for R in [0, 5, 10, 20, 40, 80, 160, 320]:\n",
    "                #for gamma in [0.0625, 0.125, 0.25, 0.5, 1.0, 2.0]:\n",
    "                #for gamma in [0.125, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 1.0]:\n",
    "                for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                    # feature mapping\n",
    "                    Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                    Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                    for C in [2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9)]:\n",
    "                        score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                        if best_cv_score > score:\n",
    "                            clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                            test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                            best_cv_score = score\n",
    "                            print(R, gamma, C, score, test_score)\n",
    "                            print(R, gamma, C, score, test_score, file=f)\n",
    "\n",
    "            return best_cv_score, test_score\n",
    "\n",
    "        print(train_and_predict(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23cc5209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "0 0.4 0.0001220703125 0.24416895871292196 0.21646611389022144\n",
      "5 0.4 0.0001220703125 0.22379115085041473 0.2137467950095606\n",
      "5 0.5 0.0001220703125 0.22379041390356877 0.2145393372632807\n",
      "10 0.4 0.0001220703125 0.22039274853576346 0.19087825345664736\n",
      "10 0.4 0.000244140625 0.21932612915891214 0.18776168114374026\n",
      "10 0.4 0.00048828125 0.2175216782012171 0.18435900861672075\n",
      "10 0.4 0.0009765625 0.2141690444325703 0.18372473134914785\n",
      "10 0.4 0.001953125 0.2116689678498397 0.18376532652126756\n",
      "20 0.4 0.0001220703125 0.19365852236495465 0.15734681996200395\n",
      "20 0.4 0.000244140625 0.19077610193472644 0.15569846007229968\n",
      "20 0.4 0.00048828125 0.19011900725624958 0.15468015639326999\n",
      "20 0.4 0.0009765625 0.18965365120374317 0.15952289448200352\n",
      "40 0.4 0.0001220703125 0.1124458877862711 0.07733258405048615\n",
      "80 0.4 0.0001220703125 0.11007161358026822 0.07953933882294051\n",
      "80 0.4 0.000244140625 0.1080271025804761 0.07244277401250349\n",
      "80 0.4 0.00048828125 0.10801547843620063 0.0750044318455023\n",
      "160 0.4 0.00048828125 0.10794918489893171 0.07371765482844504\n",
      "320 0.4 0.00048828125 0.1078012501270416 0.07504517396187224\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = -1\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=0.4, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "with open('./results/{}_data/results_{}x{}_{}_data_lasso_R=1-320_C=-13--9_gamma=0.4-1.0_delta1={}_local_feature_qubits{}.txt'.format(data_name, length, width, data_name, delta1, d), 'w') as f:\n",
    "    for (q1, q2) in qubits[7:8]:\n",
    "        print('(q1, q2) =', (q1, q2))\n",
    "        print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "        def train_and_predict():\n",
    "            # consider the pair (q1, q2)\n",
    "            global q1, q2\n",
    "\n",
    "            # training data (estimated from measurement data)\n",
    "            y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=0.4, random_state=0)\n",
    "\n",
    "            # testing data (exact expectation values)\n",
    "            y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=0.4, random_state=0)\n",
    "\n",
    "            # use cross validation to find the best hyperparameters\n",
    "            best_cv_score, test_score = 999.0, 999.0\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=10000)\n",
    "            # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "            \n",
    "            for R in [0, 5, 10, 20, 40, 80, 160, 320]:\n",
    "                #for gamma in [0.0625, 0.125, 0.25, 0.5, 1.0, 2.0]:\n",
    "                #for gamma in [0.125, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 1.0]:\n",
    "                for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                    # feature mapping\n",
    "                    Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                    Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                    for C in [2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9)]:\n",
    "                        score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                        if best_cv_score > score:\n",
    "                            clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                            test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                            best_cv_score = score\n",
    "                            print(R, gamma, C, score, test_score)\n",
    "                            print(R, gamma, C, score, test_score, file=f)\n",
    "\n",
    "            return best_cv_score, test_score\n",
    "\n",
    "        print(train_and_predict(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f5fbe241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "0.0125 0.12267339478476988 0.1037058475500417\n",
      "0.025 0.11776978090553283 0.09655039623263334\n",
      "0.05 0.11336732418216096 0.09214989065050608\n",
      "0.125 0.11131932483580997 0.10506725370706851\n",
      "8.0 0.11115347959577675 0.09004059967809445\n",
      "16.0 0.10928589398395894 0.08867342710051447\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm (Old method)\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=0.4, random_state=0)\n",
    "\n",
    "with open('./results/{}_data/results_{}x{}_{}_data_lasso_R=1-80_C=0.0125-2_delta1={}_local_feature_qubits{}.txt'.format(data_name, length, width, data_name, delta1, d), 'w') as f:\n",
    "    for (q1, q2) in qubits[7:8]:\n",
    "        print('(q1, q2) =', (q1, q2))\n",
    "        print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "        def train_and_predict():\n",
    "            # consider the pair (q1, q2)\n",
    "            global q1, q2\n",
    "\n",
    "            # training data (estimated from measurement data)\n",
    "            y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=0.4, random_state=0)\n",
    "\n",
    "            # testing data (exact expectation values)\n",
    "            y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=0.4, random_state=0)\n",
    "\n",
    "            best_cv_score, test_score = 999.0, 999.0\n",
    "            for ML_method in [(lambda Cx: svm.SVR(kernel=\"linear\", C=Cx)), (lambda Cx: KernelRidge(kernel=\"linear\", alpha=1/(2*Cx))),\\\n",
    "                              (lambda Cx: svm.SVR(kernel=\"rbf\", C=Cx)), (lambda Cx: KernelRidge(kernel=\"rbf\", alpha=1/(2*Cx)))]:\n",
    "                for C in [0.0125, 0.025, 0.05, 0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0]:\n",
    "                    score = -np.mean(cross_val_score(ML_method(C), X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                    if best_cv_score > score:\n",
    "                        clf = ML_method(C).fit(X_train, y_train.ravel())\n",
    "                        test_score = np.linalg.norm(clf.predict(X_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                        best_cv_score = score\n",
    "                        print(C, score, test_score)\n",
    "            \n",
    "            return best_cv_score, test_score\n",
    "\n",
    "        print(train_and_predict(), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
