{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d499751",
   "metadata": {},
   "source": [
    "# Predicting ground states for 2D Heisenberg models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de9d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic functionalities\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import ast\n",
    "import datetime as dt\n",
    "from timeit import default_timer as timer\n",
    "from os import path\n",
    "\n",
    "# Neural tangent kernel\n",
    "import jax\n",
    "from neural_tangents import stax\n",
    "\n",
    "# Traditional ML methods and techniques\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d17dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "length = 8 # length = 4, 5, 6, 7, 8, 9 for orig; only 4, 5, 6, 7 for new\n",
    "width = 5\n",
    "\n",
    "shadow_size = 500 # up to 1000\n",
    "\n",
    "Xfull = [] # Shape = (number of data) x (number of params)\n",
    "Ytrain = [] # Shape = (number of data) x (number of pairs), estimated 2-point correlation functions\n",
    "Yfull = [] # Shape = (number of data) x (number of pairs), exact 2-point correlation functions\n",
    "\n",
    "def get_path_prefix(data='orig'):\n",
    "    prefix = './heisenberg_data/heisenberg_{}x{}'.format(length, width)\n",
    "    if data == 'new':\n",
    "        prefix = './new_data/data_{}x{}/simulation_{}x{}'.format(length, width, length, width)\n",
    "    return prefix\n",
    "    \n",
    "data_name = 'orig'\n",
    "prefix = get_path_prefix(data=data_name)\n",
    "\n",
    "for idx in range(1, 301):\n",
    "    if path.exists('{}_id{}_XX.txt'.format(prefix, idx)) == False:\n",
    "        continue\n",
    "    with open('{}_id{}_samples.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        classical_shadow_big = [[int(c) for i, c in enumerate(line.split(\"\\t\"))] for line in f]\n",
    "        classical_shadow = classical_shadow_big[0:shadow_size]\n",
    "        for i in range(length * 5):\n",
    "            for j in range(length * 5):\n",
    "                if i == j:\n",
    "                    single_data.append(1.0)\n",
    "                    continue\n",
    "                corr = 0\n",
    "                cnt = 0\n",
    "                for shadow in classical_shadow:\n",
    "                    if shadow[i] // 2 == shadow[j] // 2:\n",
    "                        corr += 3 if shadow[i] % 2 == shadow[j] % 2 else -3\n",
    "                    cnt += 1\n",
    "                single_data.append(corr / cnt)\n",
    "        Ytrain.append(single_data)\n",
    "    with open('{}_id{}_XX.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        for line in f:\n",
    "            for i, c in enumerate(line.split(\"\\t\")):\n",
    "                v = float(c)\n",
    "                single_data.append(v)\n",
    "        Yfull.append(single_data)\n",
    "    with open('{}_id{}_couplings.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        for line in f:\n",
    "            for i, c in enumerate(line.split(\"\\t\")):\n",
    "                v = float(c)\n",
    "                single_data.append(v)\n",
    "        Xfull.append(single_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee93cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data (N) * number of params (m) = (92, 67)\n",
      "number of data (N) * number of pairs = (92, 1600)\n"
     ]
    }
   ],
   "source": [
    "# Print information\n",
    "\n",
    "Xfull = np.array(Xfull)\n",
    "print(\"number of data (N) * number of params (m) =\", Xfull.shape)\n",
    "Ytrain = np.array(Ytrain)\n",
    "Yfull = np.array(Yfull)\n",
    "print(\"number of data (N) * number of pairs =\", Yfull.shape)\n",
    "\n",
    "# print(Xfull[0])\n",
    "# print(Yfull[0].reshape((length * width, length * width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb26787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.81919825e-01  5.33921021e-01  1.32691899e-01 -7.96704066e-02\n",
      "  5.88393286e-01  7.08667985e-01 -5.98811194e-01 -4.02701000e-01\n",
      " -5.06283551e-01  1.59568894e-01  2.98026246e-01 -9.78275406e-01\n",
      " -8.67210695e-01  9.13937747e-01  2.93643108e-01 -7.75059790e-01\n",
      " -4.47900034e-01  3.03592282e-01 -8.86777190e-01  6.85795773e-01\n",
      "  9.01424407e-01  9.29774910e-01  8.91975816e-01  5.80146972e-01\n",
      "  6.42677573e-01 -9.31754228e-01 -8.10952383e-01 -3.70068172e-01\n",
      " -7.44403339e-01 -2.51514723e-01  8.62647276e-01 -1.21974758e-01\n",
      " -5.06232950e-01 -9.76447517e-01 -9.07982281e-01 -7.48397695e-03\n",
      "  4.64308646e-01 -4.01814261e-01 -1.01482847e-01  7.50578811e-01\n",
      " -9.07490317e-01  3.97000462e-01 -2.69674391e-01 -3.94971855e-01\n",
      " -2.54738153e-01 -6.98995046e-01 -7.05353677e-01 -4.33135271e-01\n",
      " -1.89965630e-01 -7.57992179e-04  3.17897355e-01  3.14437744e-02\n",
      " -4.78519868e-01  1.91274078e-01 -4.15009880e-01 -4.22775354e-01\n",
      "  2.36565108e-01  3.28790405e-01  5.07335903e-01 -9.26304587e-01\n",
      "  2.87668071e-01 -1.97032102e-01  5.03090934e-02  2.24261759e-01\n",
      " -1.34703103e-01 -8.35634063e-01 -6.01868121e-01]\n"
     ]
    }
   ],
   "source": [
    "# Normalize Xfull\n",
    "\n",
    "xmin = np.amin(Xfull)\n",
    "xmax = np.amax(Xfull)\n",
    "\n",
    "# normalize so that all entries are between -1 and 1 using min-max feature scaling\n",
    "Xfull_norm = np.array(list(map(lambda row : list(map(lambda x : -1 + 2*(x - xmin)/(xmax - xmin), row)), Xfull)))\n",
    "\n",
    "print(Xfull_norm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfbf14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 6), (1, 2), (2, 7), (2, 3), (3, 8), (3, 4), (4, 9), (4, 5), (5, 10), (6, 11), (6, 7), (7, 12), (7, 8), (8, 13), (8, 9), (9, 14), (9, 10), (10, 15), (11, 16), (11, 12), (12, 17), (12, 13), (13, 18), (13, 14), (14, 19), (14, 15), (15, 20), (16, 21), (16, 17), (17, 22), (17, 18), (18, 23), (18, 19), (19, 24), (19, 20), (20, 25), (21, 26), (21, 22), (22, 27), (22, 23), (23, 28), (23, 24), (24, 29), (24, 25), (25, 30), (26, 31), (26, 27), (27, 32), (27, 28), (28, 33), (28, 29), (29, 34), (29, 30), (30, 35), (31, 36), (31, 32), (32, 37), (32, 33), (33, 38), (33, 34), (34, 39), (34, 35), (35, 40), (36, 37), (37, 38), (38, 39), (39, 40)]\n"
     ]
    }
   ],
   "source": [
    "# Categorizing pairs of qubits by distance\n",
    "\n",
    "# grid of qubits\n",
    "grid = np.array(range(1, length * width + 1)).reshape((length, width))\n",
    "\n",
    "# generate all edges in grid in same order as Xfull\n",
    "all_edges = []\n",
    "for i in range(0, length):\n",
    "    for j in range(1, width + 1):\n",
    "        if i != length - 1:\n",
    "            all_edges.append((width * i + j, width * (i + 1) + j))\n",
    "        if j != width:\n",
    "            all_edges.append((width * i + j, width * i + j + 1))\n",
    "print(all_edges)\n",
    "            \n",
    "def calc_distance(q1, q2):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid\n",
    "    # Output l1 distance between q1 and q2 in grid\n",
    "\n",
    "    pos1 = np.array(np.where(grid == q1)).T[0]\n",
    "    pos2 = np.array(np.where(grid == q2)).T[0]\n",
    "\n",
    "    return np.abs(pos1[0] - pos2[0]) + np.abs(pos1[1] - pos2[1])\n",
    "\n",
    "def get_nearby_qubit_pairs(d):\n",
    "    # Given distance d > 0\n",
    "    # Output all pairs of qubits that are within distance d of each other\n",
    "    \n",
    "    if d == 1:\n",
    "        return all_edges\n",
    "    \n",
    "    qubit_pairs = []\n",
    "    for q1 in range(1, length * width + 1):\n",
    "        for q2 in range(1, length * width + 1):\n",
    "            dist = calc_distance(q1, q2)\n",
    "            pair = tuple(sorted((q1, q2)))\n",
    "            if dist == d and pair not in qubit_pairs:\n",
    "                qubit_pairs.append(pair)\n",
    "    \n",
    "    return qubit_pairs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9441798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding local patches of a given radius\n",
    "\n",
    "def get_local_region_qubits(q, delta1):\n",
    "    # Given a qubit q (1-indexed integer) in length x width grid and radius delta1\n",
    "    # delta1 = -1 if all qubits are in local region\n",
    "    # Output list of qubits (1-indexed integers) within a radius of delta1 of q\n",
    "    \n",
    "    if delta1 == 0:\n",
    "        return [q]\n",
    "    elif delta1 == -1:\n",
    "        return list(range(1, length * width + 1))\n",
    "    \n",
    "    local_qubits = []\n",
    "    for q2 in range(1, length * width + 1):\n",
    "        dist = calc_distance(q, q2)\n",
    "        \n",
    "        if dist <= delta1:\n",
    "            local_qubits.append(q2)\n",
    "    \n",
    "    return local_qubits\n",
    "\n",
    "def get_local_region_edges(q1, q2, delta1):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid and radius delta1\n",
    "    # delta1 = -1 if all qubits are in local region\n",
    "    # Output list of tuples of qubits (1-indexed integers) corresponding to edges in local region of radius delta1\n",
    "\n",
    "    if delta1 == 0:\n",
    "        return [(q1, q2)]\n",
    "    elif delta1 == -1:\n",
    "        return all_edges\n",
    "\n",
    "    local_qubits = list(set(get_local_region_qubits(q1, delta1) + get_local_region_qubits(q2, delta1)))\n",
    "    \n",
    "    local_edges = []\n",
    "    for edge in all_edges:\n",
    "        (q1, q2) = edge\n",
    "        if q1 in local_qubits and q2 in local_qubits:\n",
    "            local_edges.append(edge)\n",
    "\n",
    "    return local_edges\n",
    "\n",
    "def get_local_region_params(q1, q2, delta1, data, i):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid, radius delta1, and input data (i.e., Xfull)\n",
    "    # delta1 = -1 if all qubits are considered nearby\n",
    "    # Output data but only for parameters corresponding to edges within radius delta1\n",
    "    \n",
    "    edges = get_local_region_edges(q1, q2, delta1)\n",
    "    \n",
    "    indices = [all_edges.index(edge) for edge in edges]\n",
    "    \n",
    "    return np.array([data[i][j] for j in sorted(indices)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c47e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges: [(1, 6), (1, 2), (2, 7), (2, 3), (6, 7)]\n",
      "params: [ 0.18191983  0.53392102  0.1326919  -0.07967041  0.29802625]\n"
     ]
    }
   ],
   "source": [
    "print('edges: ' + str(get_local_region_edges(1,2,1)))\n",
    "print('params: ' + str(get_local_region_params(1,2,1, Xfull_norm, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5465a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature mapping\n",
    "\n",
    "def get_feature_vectors(delta1, R, data, omega, gamma=1.0, q1=0, q2=1):\n",
    "    # Given radius delta1 and hyperparameter R (number of nonlinear features per local region), input data, and fixed randomness omega\n",
    "    # delta1 = -1 if all qubits are considered nearby\n",
    "    # Output concatenated feature vectors\n",
    "    \n",
    "    # to store all concatenated feature vectors\n",
    "    all_feature_vectors = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        feature_vector_concat = []\n",
    "        # iterate over all possible local regions\n",
    "        n = len(all_edges)\n",
    "        for k in range(n):\n",
    "            (q1, q2) = all_edges[k]\n",
    "            data_local = get_local_region_params(q1, q2, delta1, data, i)\n",
    "            m_local = len(data_local)\n",
    "\n",
    "            # do nonlinear feature map on each vector in data_local\n",
    "            feature_vector = []\n",
    "\n",
    "            for j in range(R):\n",
    "                omega_j = omega[k][j]\n",
    "                val = np.exp(np.dot(omega_j, data_local) * gamma / (m_local ** 0.5) * 1j)\n",
    "                feature_vector.append(np.real(val))\n",
    "                feature_vector.append(np.imag(val))\n",
    "\n",
    "            # concatenate feature vectors together\n",
    "            feature_vector_concat += feature_vector\n",
    "            \n",
    "        all_feature_vectors.append(feature_vector_concat)\n",
    "        \n",
    "    # note all_feature_vectors is of size number of data (N) x (2 * R * number of local regions)\n",
    "    return np.array(all_feature_vectors)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0fc07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 6), (1, 2), (2, 7), (2, 3), (3, 8), (3, 4), (4, 9), (4, 5), (5, 10), (6, 11), (6, 7), (7, 12), (7, 8), (8, 13), (8, 9), (9, 14), (9, 10), (10, 15), (11, 16), (11, 12), (12, 17), (12, 13), (13, 18), (13, 14), (14, 19), (14, 15), (15, 20), (16, 21), (16, 17), (17, 22), (17, 18), (18, 23), (18, 19), (19, 24), (19, 20), (20, 25), (21, 26), (21, 22), (22, 27), (22, 23), (23, 28), (23, 24), (24, 29), (24, 25), (25, 30), (26, 31), (26, 27), (27, 32), (27, 28), (28, 33), (28, 29), (29, 34), (29, 30), (30, 35), (31, 36), (31, 32), (32, 37), (32, 33), (33, 38), (33, 34), (34, 39), (34, 35), (35, 40), (36, 37), (37, 38), (38, 39), (39, 40)]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './gdrive/MyDrive/Caltech/SURF_2022/clean_results/new_algorithm/test_size=0.5_shadow_size=500_d=1_extraruns/results_8x5_orig_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a6234f2c0d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./gdrive/MyDrive/Caltech/SURF_2022/clean_results/new_algorithm/test_size={}_shadow_size={}_d={}_extraruns'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/results_{}x{}_{}_data.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/coefficients_{}x{}_{}_data.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqubits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(q1, q2) ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './gdrive/MyDrive/Caltech/SURF_2022/clean_results/new_algorithm/test_size=0.5_shadow_size=500_d=1_extraruns/results_8x5_orig_data.txt'"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = 0\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "for d in [1]:\n",
    "    qubits = get_nearby_qubit_pairs(d)\n",
    "    print(qubits)\n",
    "\n",
    "    # set test size\n",
    "    test_size = 0.5\n",
    "\n",
    "    train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=test_size, random_state=0)\n",
    "\n",
    "    # generate omega to pass into feature mapping\n",
    "    omega = []\n",
    "    for (q1, q2) in all_edges:\n",
    "        m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "        omega_sub = []\n",
    "        for j in range(max_R):\n",
    "            omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "        omega.append(omega_sub)\n",
    "\n",
    "    data_path = './clean_results/new_algorithm/test_size={}_shadow_size={}_d={}_extraruns'.format(test_size, shadow_size, d)\n",
    "    with open('{}/results_{}x{}_{}_data.txt'.format(data_path, length, width, data_name), 'w') as f1, open('{}/coefficients_{}x{}_{}_data.txt'.format(data_path, length, width, data_name), 'w') as f2:\n",
    "        for (q1, q2) in qubits:\n",
    "            print('(q1, q2) =', (q1, q2))\n",
    "            print('(q1, q2) =', (q1, q2), file=f1)\n",
    "            print('(q1, q2) =', (q1, q2), file=f2)\n",
    "\n",
    "            def train_and_predict():\n",
    "                # consider the pair (q1, q2)\n",
    "                global q1, q2\n",
    "\n",
    "                # training data (estimated from measurement data)\n",
    "                y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "                X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=test_size, random_state=0)\n",
    "\n",
    "                # testing data (exact expectation values)\n",
    "                y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "                _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=test_size, random_state=0)\n",
    "\n",
    "                # use cross validation to find the best hyperparameters\n",
    "                best_cv_score, test_score = 999.0, 999.0\n",
    "                ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=10000)\n",
    "                best_coef = []\n",
    "                #ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "\n",
    "                for R in [5, 10, 20, 40]:\n",
    "                    for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75]:\n",
    "                        # feature mapping\n",
    "                        Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                        Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                        for alpha in [2**(-8), 2**(-7), 2**(-6), 2**(-5)]:\n",
    "                            score = -np.mean(cross_val_score(ML_method(alpha), Xfeature_train, y_train, cv=4, scoring=\"neg_root_mean_squared_error\"))\n",
    "                            if best_cv_score > score:\n",
    "                                clf = ML_method(alpha).fit(Xfeature_train, y_train.ravel())\n",
    "                                test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                                best_cv_score = score\n",
    "                                best_coef = clf.coef_.reshape((len(all_edges), 2 * R))\n",
    "\n",
    "                                #coef = clf.coef_.reshape((len(all_edges), 2 * R))\n",
    "                                #print(list(zip(all_edges, np.linalg.norm(coef, axis=1))))\n",
    "                                #print(R, gamma, alpha, score, test_score)\n",
    "\n",
    "                coef_edges = list(zip(all_edges, np.linalg.norm(best_coef, axis=1)))\n",
    "                return best_cv_score, test_score, coef_edges\n",
    "\n",
    "            print(train_and_predict()[0:2], file=f1)\n",
    "            print(train_and_predict()[2], file=f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3b8b3d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructed Dirichlet kernel\n",
      "constructed neural tangent kernel\n",
      "RBF kernel (will be constructed in sklearn)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Dirichlet kernel\n",
    "#\n",
    "\n",
    "kernel_dir = np.zeros((len(Xfull), Xfull.shape[1]*5))\n",
    "for i, x1 in enumerate(Xfull):\n",
    "    cnt = 0\n",
    "    for k in range(len(x1)):\n",
    "        for k1 in range(-2, 3):\n",
    "            kernel_dir[i, cnt] += np.cos(np.pi * k1 * x1[k])\n",
    "            cnt += 1\n",
    "print(\"constructed Dirichlet kernel\")\n",
    "            \n",
    "#\n",
    "# Neural tangent kernel\n",
    "#\n",
    "    \n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(1)\n",
    ")\n",
    "kernel_NN2 = kernel_fn(Xfull, Xfull, 'ntk')\n",
    "\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(1)\n",
    ")\n",
    "kernel_NN3 = kernel_fn(Xfull, Xfull, 'ntk')\n",
    "                \n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(1)\n",
    ")\n",
    "kernel_NN4 = kernel_fn(Xfull, Xfull, 'ntk')\n",
    "\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(1)\n",
    ")\n",
    "kernel_NN5 = kernel_fn(Xfull, Xfull, 'ntk')\n",
    "\n",
    "list_kernel_NN = [kernel_NN2, kernel_NN3, kernel_NN4, kernel_NN5]\n",
    "\n",
    "for r in range(len(list_kernel_NN)):\n",
    "    kernel = list_kernel_NN[r].copy()\n",
    "    for i in range(len(list_kernel_NN[r])):\n",
    "        for j in range(len(list_kernel_NN[r])):\n",
    "            # list_kernel_NN[r][i][j] /= (list_kernel_NN[r][i][i] * list_kernel_NN[r][j][j]) ** 0.5\n",
    "            list_kernel_NN[r][i].at[j].divide((kernel[i][i] * kernel[j][j]) ** 0.5)\n",
    "print(\"constructed neural tangent kernel\")\n",
    "            \n",
    "#\n",
    "# RBF kernel is defined in Sklearn\n",
    "#\n",
    "print(\"RBF kernel (will be constructed in sklearn)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "11d497a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "Dirich. kernel (0.2524725471445278, 0.24720161876210964)\n",
      "Gaussi. kernel (0.18580648811215386, 0.19334888028240627)\n",
      "Neur. T kernel (0.13558781374889525, 0.1640358656018234)\n",
      "Neur. T kernel (0.1397908436522027, 0.16704989523840308)\n",
      "Neur. T kernel (0.16193101833276144, 0.18731811705888335)\n",
      "Neur. T kernel (0.19950551325783114, 0.21130932478677994)\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm (Old method)\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "test_size = 0.4\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=test_size, random_state=0)\n",
    "\n",
    "#with open('./results/orig_algorithm/orig_algorithm_test_size={}/orig_algorithm_{}_data_k/results_{}x{}_all_qubits.txt'.format(test_size, data_name, length, width), 'w') as f:\n",
    "for (q1, q2) in qubits[7:8]:\n",
    "    # each k corresponds to the correlation function in a pair of qubits\n",
    "    print(\"(q1, q2) = ({}, {})\".format(q1, q2))\n",
    "    #print(\"k =\", k, file=f)\n",
    "\n",
    "    def train_and_predict(kernel, opt=\"linear\"): # opt=\"linear\" or \"rbf\"\n",
    "\n",
    "        # instance-wise normalization\n",
    "        for i in range(len(kernel)):\n",
    "            if type(kernel) == np.ndarray:\n",
    "                kernel[i] /= np.linalg.norm(kernel[i])\n",
    "            else:\n",
    "                kernel.at[i].divide(np.linalg.norm(kernel[i]))\n",
    "\n",
    "        # consider the k-th pair\n",
    "        global q1, q2\n",
    "\n",
    "        # training data (estimated from measurement data)\n",
    "        y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(kernel, y, test_size=test_size, random_state=0)\n",
    "\n",
    "        # testing data (exact expectation values)\n",
    "        y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "        _, _, _, y_test_clean = train_test_split(kernel, y_clean, test_size=test_size, random_state=0)\n",
    "\n",
    "        # use cross validation to find the best method + hyper-param\n",
    "        best_cv_score, test_score = 999.0, 999.0\n",
    "        for ML_method in [(lambda Cx: svm.SVR(kernel=opt, C=Cx)), (lambda Cx: KernelRidge(kernel=opt, alpha=1/(2*Cx)))]:\n",
    "            for C in [0.0125, 0.025, 0.05, 0.125, 0.25, 0.5, 1.0, 2.0]:\n",
    "                score = -np.mean(cross_val_score(ML_method(C), X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(C).fit(X_train, y_train.ravel())\n",
    "                    test_score = np.linalg.norm(clf.predict(X_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                    best_cv_score = score\n",
    "\n",
    "        return best_cv_score, test_score\n",
    "\n",
    "    # Dirichlet\n",
    "    #print(\"Dirich. kernel\", train_and_predict(kernel_dir), file=f)\n",
    "    print(\"Dirich. kernel\", train_and_predict(kernel_dir))\n",
    "    # RBF\n",
    "    #print(\"Gaussi. kernel\", train_and_predict(Xfull, opt=\"rbf\"), file=f)\n",
    "    print(\"Gaussi. kernel\", train_and_predict(Xfull, opt=\"rbf\"))\n",
    "    # Neural tangent\n",
    "    for kernel_NN in list_kernel_NN:\n",
    "        #print(\"Neur. T kernel\", train_and_predict(kernel_NN), file=f)\n",
    "        print(\"Neur. T kernel\", train_and_predict(kernel_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790a8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
