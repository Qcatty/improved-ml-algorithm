{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d499751",
   "metadata": {},
   "source": [
    "# Predicting ground states for 2D Heisenberg models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de9d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic functionalities\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import ast\n",
    "import datetime as dt\n",
    "from timeit import default_timer as timer\n",
    "from os import path\n",
    "\n",
    "# Neural tangent kernel\n",
    "import jax\n",
    "from neural_tangents import stax\n",
    "\n",
    "# Traditional ML methods and techniques\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66d17dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "length = 7 # length = 4, 5, 6, 7, 8, 9 for orig; only 4, 5, 6, 7 for new\n",
    "width = 5\n",
    "\n",
    "shadow_size = 50 # up to 1000\n",
    "\n",
    "Xfull = [] # Shape = (number of data) x (number of params)\n",
    "Ytrain = [] # Shape = (number of data) x (number of pairs), estimated 2-point correlation functions\n",
    "Yfull = [] # Shape = (number of data) x (number of pairs), exact 2-point correlation functions\n",
    "\n",
    "def get_path_prefix(data='orig'):\n",
    "    prefix = './heisenberg_data/heisenberg_{}x{}'.format(length, width)\n",
    "    if data == 'new':\n",
    "        prefix = './new_data/data_{}x{}/simulation_{}x{}'.format(length, width, length, width)\n",
    "    return prefix\n",
    "    \n",
    "data_name = 'new'\n",
    "prefix = get_path_prefix(data=data_name)\n",
    "\n",
    "for idx in range(1, 301):\n",
    "    if path.exists('{}_id{}_XX.txt'.format(prefix, idx)) == False:\n",
    "        continue\n",
    "    with open('{}_id{}_samples.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        classical_shadow_big = [[int(c) for i, c in enumerate(line.split(\"\\t\"))] for line in f]\n",
    "        classical_shadow = classical_shadow_big[0:shadow_size]\n",
    "        for i in range(length * 5):\n",
    "            for j in range(length * 5):\n",
    "                if i == j:\n",
    "                    single_data.append(1.0)\n",
    "                    continue\n",
    "                corr = 0\n",
    "                cnt = 0\n",
    "                for shadow in classical_shadow:\n",
    "                    if shadow[i] // 2 == shadow[j] // 2:\n",
    "                        corr += 3 if shadow[i] % 2 == shadow[j] % 2 else -3\n",
    "                    cnt += 1\n",
    "                single_data.append(corr / cnt)\n",
    "        Ytrain.append(single_data)\n",
    "    with open('{}_id{}_XX.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        for line in f:\n",
    "            for i, c in enumerate(line.split(\"\\t\")):\n",
    "                v = float(c)\n",
    "                single_data.append(v)\n",
    "        Yfull.append(single_data)\n",
    "    with open('{}_id{}_couplings.txt'.format(prefix, idx), 'r') as f:\n",
    "        single_data = []\n",
    "        for line in f:\n",
    "            for i, c in enumerate(line.split(\"\\t\")):\n",
    "                v = float(c)\n",
    "                single_data.append(v)\n",
    "        Xfull.append(single_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bee93cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data (N) * number of params (m) = (300, 58)\n",
      "number of data (N) * number of pairs = (300, 1225)\n"
     ]
    }
   ],
   "source": [
    "# Print information\n",
    "\n",
    "Xfull = np.array(Xfull)\n",
    "print(\"number of data (N) * number of params (m) =\", Xfull.shape)\n",
    "Ytrain = np.array(Ytrain)\n",
    "Yfull = np.array(Yfull)\n",
    "print(\"number of data (N) * number of pairs =\", Yfull.shape)\n",
    "\n",
    "# print(Xfull[0])\n",
    "# print(Yfull[0].reshape((length * width, length * width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "edb26787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02014906  0.2717357  -0.06096961 -0.23706368  0.31690494  0.41663855\n",
      " -0.66754314 -0.50492558 -0.59081781 -0.0386828   0.07612815 -0.98220066\n",
      " -0.89010411  0.58685137  0.07249359 -0.81369117 -0.5424053   0.0807436\n",
      " -0.90632894  0.39767257  0.57647512  0.59998379  0.56864021  0.31006697\n",
      "  0.3619183  -0.94362459 -0.84345385 -0.47786594 -0.78827037 -0.37955961\n",
      "  0.54432053]\n"
     ]
    }
   ],
   "source": [
    "# Normalize Xfull\n",
    "\n",
    "xmin = np.amin(Xfull)\n",
    "xmax = np.amax(Xfull)\n",
    "\n",
    "# normalize so that all entries are between -1 and 1 using min-max feature scaling\n",
    "Xfull_norm = np.array(list(map(lambda row : list(map(lambda x : -1 + 2*(x - xmin)/(xmax - xmin), row)), Xfull)))\n",
    "\n",
    "print(Xfull_norm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "abfbf14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 6), (1, 2), (2, 7), (2, 3), (3, 8), (3, 4), (4, 9), (4, 5), (5, 10), (6, 11), (6, 7), (7, 12), (7, 8), (8, 13), (8, 9), (9, 14), (9, 10), (10, 15), (11, 16), (11, 12), (12, 17), (12, 13), (13, 18), (13, 14), (14, 19), (14, 15), (15, 20), (16, 17), (17, 18), (18, 19), (19, 20)]\n"
     ]
    }
   ],
   "source": [
    "# Categorizing pairs of qubits by distance\n",
    "\n",
    "# grid of qubits\n",
    "grid = np.array(range(1, length * width + 1)).reshape((length, width))\n",
    "\n",
    "# generate all edges in grid in same order as Xfull\n",
    "all_edges = []\n",
    "for i in range(0, length):\n",
    "    for j in range(1, width + 1):\n",
    "        if i != length - 1:\n",
    "            all_edges.append((width * i + j, width * (i + 1) + j))\n",
    "        if j != width:\n",
    "            all_edges.append((width * i + j, width * i + j + 1))\n",
    "print(all_edges)\n",
    "            \n",
    "def calc_distance(q1, q2):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid\n",
    "    # Output l1 distance between q1 and q2 in grid\n",
    "\n",
    "    pos1 = np.array(np.where(grid == q1)).T[0]\n",
    "    pos2 = np.array(np.where(grid == q2)).T[0]\n",
    "\n",
    "    return np.abs(pos1[0] - pos2[0]) + np.abs(pos1[1] - pos2[1])\n",
    "\n",
    "def get_nearby_qubit_pairs(d):\n",
    "    # Given distance d > 0\n",
    "    # Output all pairs of qubits that are within distance d of each other\n",
    "    \n",
    "    if d == 1:\n",
    "        return all_edges\n",
    "    \n",
    "    qubit_pairs = []\n",
    "    for q1 in range(1, length * width + 1):\n",
    "        for q2 in range(1, length * width + 1):\n",
    "            dist = calc_distance(q1, q2)\n",
    "            pair = tuple(sorted((q1, q2)))\n",
    "            if dist == d and pair not in qubit_pairs:\n",
    "                qubit_pairs.append(pair)\n",
    "    \n",
    "    return qubit_pairs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9441798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding local patches of a given radius\n",
    "\n",
    "def get_local_region_qubits(q, delta1):\n",
    "    # Given a qubit q (1-indexed integer) in length x width grid and radius delta1\n",
    "    # delta1 = -1 if all qubits are in local region\n",
    "    # Output list of qubits (1-indexed integers) within a radius of delta1 of q\n",
    "    \n",
    "    if delta1 == 0:\n",
    "        return [q]\n",
    "    elif delta1 == -1:\n",
    "        return list(range(1, length * width + 1))\n",
    "    \n",
    "    local_qubits = []\n",
    "    for q2 in range(1, length * width + 1):\n",
    "        dist = calc_distance(q, q2)\n",
    "        \n",
    "        if dist <= delta1:\n",
    "            local_qubits.append(q2)\n",
    "    \n",
    "    return local_qubits\n",
    "\n",
    "def get_local_region_edges(q1, q2, delta1):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid and radius delta1\n",
    "    # delta1 = -1 if all qubits are in local region\n",
    "    # Output list of tuples of qubits (1-indexed integers) corresponding to edges in local region of radius delta1\n",
    "\n",
    "    if delta1 == 0:\n",
    "        return [(q1, q2)]\n",
    "    elif delta1 == -1:\n",
    "        return all_edges\n",
    "\n",
    "    local_qubits = list(set(get_local_region_qubits(q1, delta1) + get_local_region_qubits(q2, delta1)))\n",
    "    \n",
    "    local_edges = []\n",
    "    for edge in all_edges:\n",
    "        (q1, q2) = edge\n",
    "        if q1 in local_qubits and q2 in local_qubits:\n",
    "            local_edges.append(edge)\n",
    "\n",
    "    return local_edges\n",
    "\n",
    "def get_local_region_params(q1, q2, delta1, data, i):\n",
    "    # Given two qubits q1, q2 (1-indexed integers) in length x width grid, radius delta1, and input data (i.e., Xfull)\n",
    "    # delta1 = -1 if all qubits are considered nearby\n",
    "    # Output data but only for parameters corresponding to edges within radius delta1\n",
    "    \n",
    "    edges = get_local_region_edges(q1, q2, delta1)\n",
    "    \n",
    "    indices = [all_edges.index(edge) for edge in edges]\n",
    "    \n",
    "    return np.array([data[i][j] for j in sorted(indices)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c47e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges: [(1, 6), (1, 2), (2, 7), (2, 3), (6, 7)]\n",
      "params: [ 0.18167065  0.53366212  0.13244408 -0.07991235  0.29777386]\n"
     ]
    }
   ],
   "source": [
    "print('edges: ' + str(get_local_region_edges(1,2,1)))\n",
    "print('params: ' + str(get_local_region_params(1,2,1, Xfull_norm, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5465a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature mapping\n",
    "\n",
    "def get_feature_vectors(delta1, R, data, omega, gamma=1.0, q1=0, q2=1):\n",
    "    # Given radius delta1 and hyperparameter R (number of nonlinear features per local region), input data, and fixed randomness omega\n",
    "    # delta1 = -1 if all qubits are considered nearby\n",
    "    # Output concatenated feature vectors\n",
    "    \n",
    "    # to store all concatenated feature vectors\n",
    "    all_feature_vectors = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        feature_vector_concat = []\n",
    "        # iterate over all possible local regions\n",
    "        n = len(all_edges)\n",
    "        for k in range(n):\n",
    "            (q1, q2) = all_edges[k]\n",
    "            data_local = get_local_region_params(q1, q2, delta1, data, i)\n",
    "            m_local = len(data_local)\n",
    "\n",
    "            # do nonlinear feature map on each vector in data_local\n",
    "            feature_vector = []\n",
    "\n",
    "            for j in range(R):\n",
    "                omega_j = omega[k][j]\n",
    "                val = np.exp(np.dot(omega_j, data_local) * gamma / (m_local ** 0.5) * 1j)\n",
    "                feature_vector.append(np.real(val))\n",
    "                feature_vector.append(np.imag(val))\n",
    "\n",
    "            # concatenate feature vectors together\n",
    "            feature_vector_concat += feature_vector\n",
    "            \n",
    "        all_feature_vectors.append(feature_vector_concat)\n",
    "        \n",
    "    # note all_feature_vectors is of size number of data (N) x (2 * R * number of local regions)\n",
    "    return np.array(all_feature_vectors)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ff0fc07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (5, 10)\n",
      "5\n",
      "[((1, 6), 0.053595340814637724), ((1, 2), 0.04021351626476746), ((2, 7), 0.027281502178224573), ((2, 3), 0.03504675567317304), ((3, 8), 0.07688562508428554), ((3, 4), 0.13421304959663102), ((4, 9), 0.9640757267263806), ((4, 5), 0.21396885738837754), ((5, 10), 0.7527127392111249), ((6, 11), 0.002470349890239215), ((6, 7), 0.09935371714121025), ((7, 12), 0.12484990488569246), ((7, 8), 0.02335888727720967), ((8, 13), 0.013712033765945372), ((8, 9), 0.12175700084048097), ((9, 14), 0.10077795163695377), ((9, 10), 0.278929969514324), ((10, 15), 0.7088487963730341), ((11, 16), 0.053107136859272146), ((11, 12), 0.1743569465120368), ((12, 17), 0.040002687523060014), ((12, 13), 0.01805395918249298), ((13, 18), 0.09778756993565206), ((13, 14), 0.05054445042407667), ((14, 19), 0.0036353624842403224), ((14, 15), 0.005943804882595105), ((15, 20), 0.1653777605336501), ((16, 17), 0.009519410131453751), ((17, 18), 0.17011973842283118), ((18, 19), 0.0), ((19, 20), 0.13505739453885748)]\n",
      "5 0.4 0.000244140625 0.15871847979372006 0.10435925150198552\n",
      "[((1, 6), 0.04834477242099663), ((1, 2), 0.008433812940230762), ((2, 7), 0.022969606759271646), ((2, 3), 0.0006802741721312176), ((3, 8), 0.06531730577664929), ((3, 4), 0.10011688638898367), ((4, 9), 0.3947212224101592), ((4, 5), 0.20188070368689895), ((5, 10), 0.7368094238901965), ((6, 11), 0.0), ((6, 7), 0.08072858135630916), ((7, 12), 0.02987830164081297), ((7, 8), 0.021039826931632647), ((8, 13), 0.030088512040374556), ((8, 9), 0.11555782993991819), ((9, 14), 0.0838976449336923), ((9, 10), 0.23521181883274117), ((10, 15), 0.26037001434520024), ((11, 16), 0.019346889639367374), ((11, 12), 0.12656303609032), ((12, 17), 0.024602919350333072), ((12, 13), 0.018165581180295626), ((13, 18), 0.056703352952356546), ((13, 14), 0.0), ((14, 19), 0.0), ((14, 15), 0.010455407055079033), ((15, 20), 0.14484833881095674), ((16, 17), 0.015338039680847232), ((17, 18), 0.09376832224447279), ((18, 19), 0.0), ((19, 20), 0.05203806659830356)]\n",
      "5 0.4 0.00048828125 0.1469914003784851 0.08934563166287088\n",
      "[((1, 6), 0.04211797610275081), ((1, 2), 0.0), ((2, 7), 0.0), ((2, 3), 0.0), ((3, 8), 0.06977710109013291), ((3, 4), 0.08942599551624764), ((4, 9), 0.19869237313486704), ((4, 5), 0.19415008082250343), ((5, 10), 0.6922288861661189), ((6, 11), 0.001140027094788073), ((6, 7), 0.05909284663070151), ((7, 12), 0.02192294296565042), ((7, 8), 0.0), ((8, 13), 0.027126775956159924), ((8, 9), 0.10096754577002127), ((9, 14), 0.062446559900036815), ((9, 10), 0.19248056338991645), ((10, 15), 0.2518763274843831), ((11, 16), 0.0), ((11, 12), 0.024364781600003667), ((12, 17), 0.022331434628182444), ((12, 13), 0.0), ((13, 18), 0.01817564057935189), ((13, 14), 0.0), ((14, 19), 0.0), ((14, 15), 0.00467462622514248), ((15, 20), 0.1375740847329133), ((16, 17), 0.00017656541387645557), ((17, 18), 0.07019355807279777), ((18, 19), 0.0), ((19, 20), 0.038949778305741)]\n",
      "5 0.4 0.0009765625 0.13395463461391455 0.08536471178495222\n",
      "[((1, 6), 0.01598974910821058), ((1, 2), 0.0), ((2, 7), 0.008140923779773417), ((2, 3), 0.0034106070763486335), ((3, 8), 0.045764067446401546), ((3, 4), 0.062390514633550526), ((4, 9), 0.19356477868735333), ((4, 5), 0.17773331293308603), ((5, 10), 0.6560051141625463), ((6, 11), 0.000289164010735107), ((6, 7), 0.02062640205017208), ((7, 12), 0.010524639188785377), ((7, 8), 0.0), ((8, 13), 0.0), ((8, 9), 0.06627933810711623), ((9, 14), 0.024524425846500515), ((9, 10), 0.15359462767870738), ((10, 15), 0.24859553890087926), ((11, 16), 0.0), ((11, 12), 0.016828630811958935), ((12, 17), 0.0), ((12, 13), 0.0), ((13, 18), 0.0), ((13, 14), 0.0), ((14, 19), 0.0), ((14, 15), 0.0), ((15, 20), 0.11557100663707051), ((16, 17), 0.0), ((17, 18), 0.03644335453730986), ((18, 19), 0.0), ((19, 20), 0.020885929620191424)]\n",
      "5 0.4 0.001953125 0.1266362686749974 0.08647196805580828\n",
      "[((1, 6), 0.0), ((1, 2), 0.0), ((2, 7), 0.009626182960897546), ((2, 3), 0.00976021937468142), ((3, 8), 0.01901194516865576), ((3, 4), 0.0254010985713644), ((4, 9), 0.17246035135982826), ((4, 5), 0.15374032175831198), ((5, 10), 0.6185500139468272), ((6, 11), 0.0), ((6, 7), 0.0), ((7, 12), 0.0), ((7, 8), 0.0), ((8, 13), 0.0), ((8, 9), 0.007681307130390485), ((9, 14), 0.0), ((9, 10), 0.09841835258472366), ((10, 15), 0.24046348755307542), ((11, 16), 0.0), ((11, 12), 0.005392607758626134), ((12, 17), 0.0), ((12, 13), 0.0), ((13, 18), 0.0), ((13, 14), 0.0), ((14, 19), 0.0), ((14, 15), 0.0), ((15, 20), 0.08531532806943944), ((16, 17), 0.0), ((17, 18), 0.0), ((18, 19), 0.0), ((19, 20), 0.0)]\n",
      "5 0.4 0.00390625 0.12024313937654615 0.10084700412799302\n",
      "10\n",
      "[((1, 6), 0.0), ((1, 2), 0.0), ((2, 7), 0.016996912589582098), ((2, 3), 0.0001752197374591904), ((3, 8), 0.02657856170203005), ((3, 4), 0.03868602665220715), ((4, 9), 0.1426132871158461), ((4, 5), 0.1471909750010722), ((5, 10), 0.6020171578093372), ((6, 11), 0.0), ((6, 7), 0.0), ((7, 12), 0.0), ((7, 8), 0.0), ((8, 13), 0.0), ((8, 9), 0.006398983859414505), ((9, 14), 0.0), ((9, 10), 0.08421482783783277), ((10, 15), 0.21613764335111832), ((11, 16), 0.0), ((11, 12), 0.0), ((12, 17), 0.0), ((12, 13), 0.0), ((13, 18), 0.0), ((13, 14), 0.0), ((14, 19), 0.0), ((14, 15), 0.0), ((15, 20), 0.07104536082585024), ((16, 17), 0.0), ((17, 18), 0.02145220032585998), ((18, 19), 0.0), ((19, 20), 0.0)]\n",
      "10 0.4 0.00390625 0.11680486163445818 0.09732087847091138\n",
      "20\n",
      "[((1, 6), 0.0), ((1, 2), 0.0), ((2, 7), 0.0), ((2, 3), 0.0), ((3, 8), 0.0), ((3, 4), 0.0), ((4, 9), 0.11370719710950812), ((4, 5), 0.12691303684426744), ((5, 10), 0.4025278883627468), ((6, 11), 0.0), ((6, 7), 0.0), ((7, 12), 0.0), ((7, 8), 0.0), ((8, 13), 0.0), ((8, 9), 0.0), ((9, 14), 0.0), ((9, 10), 0.04864853450134831), ((10, 15), 0.1507623552985401), ((11, 16), 0.0), ((11, 12), 0.0), ((12, 17), 0.0), ((12, 13), 0.0), ((13, 18), 0.0), ((13, 14), 0.0), ((14, 19), 0.0), ((14, 15), 0.0), ((15, 20), 0.04855318892528589), ((16, 17), 0.0), ((17, 18), 0.0), ((18, 19), 0.0), ((19, 20), 0.0)]\n",
      "20 0.4 0.0078125 0.11351197141361631 0.10909636795588859\n",
      "40\n",
      "80\n",
      "(0.11351197141361631, 0.10909636795588859)\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = 0\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "# set test size\n",
    "test_size = 0.4\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=test_size, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "#with open('./results/{}_data_shadow_size={}/results_{}x{}_{}_data_lasso_R=0-320_C=-13--9_gamma=0.4-1.0_delta1={}_local_feature_qubits{}.txt'.format(data_name, shadow_size, length, width, data_name, delta1, d), 'w') as f:\n",
    "for (q1, q2) in qubits[8:9]:\n",
    "    print('(q1, q2) =', (q1, q2))\n",
    "    #print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "    def train_and_predict():\n",
    "        # consider the pair (q1, q2)\n",
    "        global q1, q2\n",
    "\n",
    "        # training data (estimated from measurement data)\n",
    "        y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=test_size, random_state=0)\n",
    "\n",
    "        # testing data (exact expectation values)\n",
    "        y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "        _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=test_size, random_state=0)\n",
    "\n",
    "        # use cross validation to find the best hyperparameters\n",
    "        best_cv_score, test_score = 999.0, 999.0\n",
    "        ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=30000)\n",
    "        # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "\n",
    "        for R in [5, 10, 20, 40, 80]:\n",
    "            print(R)\n",
    "            for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                # feature mapping\n",
    "                Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                for C in [2**(-12), 2**(-11), 2**(-10), 2**(-9), 2**(-8), 2**(-7)]:\n",
    "                    score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                    if best_cv_score > score:\n",
    "                        clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                        test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                        best_cv_score = score\n",
    "                        #print(clf.coef_)\n",
    "                        coef = clf.coef_.reshape((len(all_edges), 2 * R))\n",
    "                        \n",
    "                        print(list(zip(all_edges, np.linalg.norm(coef, axis=1))))\n",
    "                        print(R, gamma, C, score, test_score)\n",
    "                        #print(R, gamma, C, score, test_score, file=f)\n",
    "\n",
    "        return best_cv_score, test_score\n",
    "\n",
    "    #print(train_and_predict(), file=f)\n",
    "    print(train_and_predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e1276aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "0 0.4 0.0001220703125 0.249689886220962 0.24969465470979296\n",
      "5 0.4 0.0001220703125 0.14271310672603807 0.14047354251508498\n",
      "5 0.4 0.000244140625 0.1314114257197012 0.12540542819407136\n",
      "5 0.4 0.00048828125 0.12123482352749315 0.12209969250668187\n",
      "5 0.4 0.0009765625 0.11314618346030228 0.12102611279360974\n",
      "5 0.5 0.001953125 0.11292876186530872 0.11997474710373347\n",
      "10 0.4 0.0009765625 0.11177700164581035 0.11878038952934031\n",
      "10 0.4 0.001953125 0.10952901389655374 0.11398372710787537\n",
      "20 0.4 0.001953125 0.1065944023340896 0.11180554432302076\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-f158eeaf61c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m#print(train_and_predict(), file=f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-206-f158eeaf61c3>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mC\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mML_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXfeature_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neg_root_mean_squared_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbest_cv_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mML_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXfeature_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[1;32m    446\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    248\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[1;32m    249\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[0;32m--> 250\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    251\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    252\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mthis_Xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_dual_gap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_iter\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m                 self.path(X, y[:, k],\n\u001b[0m\u001b[1;32m    845\u001b[0m                           \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                           \u001b[0mn_alphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 tol, rng, random, positive)\n\u001b[1;32m    529\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mprecompute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             model = cd_fast.enet_coordinate_descent(\n\u001b[0m\u001b[1;32m    531\u001b[0m                 \u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 positive)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = 1\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "# set test size\n",
    "test_size = 0.4\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=test_size, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "#with open('./results/{}_data_shadow_size={}/results_{}x{}_{}_data_lasso_R=0-320_C=-13--9_gamma=0.4-1.0_delta1={}_local_feature_qubits{}.txt'.format(data_name, shadow_size, length, width, data_name, delta1, d), 'w') as f:\n",
    "for (q1, q2) in qubits[7:8]:\n",
    "    print('(q1, q2) =', (q1, q2))\n",
    "    #print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "    def train_and_predict():\n",
    "        # consider the pair (q1, q2)\n",
    "        global q1, q2\n",
    "\n",
    "        # training data (estimated from measurement data)\n",
    "        y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=test_size, random_state=0)\n",
    "\n",
    "        # testing data (exact expectation values)\n",
    "        y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "        _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=test_size, random_state=0)\n",
    "\n",
    "        # use cross validation to find the best hyperparameters\n",
    "        best_cv_score, test_score = 999.0, 999.0\n",
    "        ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=100000)\n",
    "        # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "\n",
    "        for R in [0, 5, 10, 20, 40, 80, 160, 320]:\n",
    "            for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                # feature mapping\n",
    "                Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                for C in [2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9)]:\n",
    "                    score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                    if best_cv_score > score:\n",
    "                        clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                        test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                        best_cv_score = score\n",
    "                        print(R, gamma, C, score, test_score)\n",
    "                        #print(R, gamma, C, score, test_score, file=f)\n",
    "\n",
    "        return best_cv_score, test_score\n",
    "\n",
    "    #print(train_and_predict(), file=f)\n",
    "    print(train_and_predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7ef50a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "0 0.4 0.0001220703125 0.25199203412263527 0.25619912147907437\n",
      "5 0.4 0.0009765625 0.24720200952473928 0.15459052582642505\n",
      "5 0.4 0.001953125 0.22952820276951585 0.14151391504496996\n",
      "5 0.6 0.001953125 0.2276538246085466 0.15845341398706472\n",
      "10 0.75 0.0009765625 0.22740041212569445 0.1424694908043115\n",
      "10 1.0 0.0009765625 0.22301038506632692 0.15385457775371217\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = 2\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "# set test size\n",
    "test_size = 0.4\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=test_size, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "with open('./results/{}_data_shadow_size={}/results_{}x{}_{}_data_lasso_R=0-320_C=-13--9_gamma=0.4-1.0_delta1={}_local_feature_qubits{}.txt'.format(data_name, shadow_size, length, width, data_name, delta1, d), 'w') as f:\n",
    "    for (q1, q2) in qubits[7:8]:\n",
    "        print('(q1, q2) =', (q1, q2))\n",
    "        print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "        def train_and_predict():\n",
    "            # consider the pair (q1, q2)\n",
    "            global q1, q2\n",
    "\n",
    "            # training data (estimated from measurement data)\n",
    "            y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=test_size, random_state=0)\n",
    "\n",
    "            # testing data (exact expectation values)\n",
    "            y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=test_size, random_state=0)\n",
    "\n",
    "            # use cross validation to find the best hyperparameters\n",
    "            best_cv_score, test_score = 999.0, 999.0\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=100000)\n",
    "            # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "            \n",
    "            for R in [0, 5, 10, 20, 40, 80, 160, 320]:\n",
    "                for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                    # feature mapping\n",
    "                    Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                    Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                    for C in [2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9)]:\n",
    "                        score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                        if best_cv_score > score:\n",
    "                            clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                            test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                            best_cv_score = score\n",
    "                            print(R, gamma, C, score, test_score)\n",
    "                            print(R, gamma, C, score, test_score, file=f)\n",
    "\n",
    "            return best_cv_score, test_score\n",
    "\n",
    "        print(train_and_predict(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "23cc5209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "0 0.4 0.0001220703125 0.25199203412263527 0.25619912147907437\n",
      "5 0.4 0.001953125 0.24501113107857425 0.15218067682718503\n",
      "10 0.4 0.001953125 0.24017175517430428 0.13937621462981103\n",
      "320 0.75 0.001953125 0.23977756736623013 0.15168143810555285\n",
      "320 1.0 0.0001220703125 0.2289171505966186 0.1635257364368335\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "\n",
    "# set size of local region\n",
    "delta1 = -1\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "# set test size\n",
    "test_size = 0.4\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=test_size, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "with open('./results/{}_data_shadow_size={}/results_{}x{}_{}_data_lasso_R=0-320_C=-13--9_gamma=0.4-1.0_delta1={}_local_feature_qubits{}.txt'.format(data_name, shadow_size, length, width, data_name, delta1, d), 'w') as f:\n",
    "    for (q1, q2) in qubits[7:8]:\n",
    "        print('(q1, q2) =', (q1, q2))\n",
    "        print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "        def train_and_predict():\n",
    "            # consider the pair (q1, q2)\n",
    "            global q1, q2\n",
    "\n",
    "            # training data (estimated from measurement data)\n",
    "            y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=test_size, random_state=0)\n",
    "\n",
    "            # testing data (exact expectation values)\n",
    "            y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=test_size, random_state=0)\n",
    "\n",
    "            # use cross validation to find the best hyperparameters\n",
    "            best_cv_score, test_score = 999.0, 999.0\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=10000)\n",
    "            # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "            \n",
    "            for R in [0, 5, 10, 20, 40, 80, 160, 320]:\n",
    "                #for gamma in [0.0625, 0.125, 0.25, 0.5, 1.0, 2.0]:\n",
    "                #for gamma in [0.125, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 1.0]:\n",
    "                for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                    # feature mapping\n",
    "                    Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                    Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                    for C in [2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9)]:\n",
    "                        score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                        if best_cv_score > score:\n",
    "                            clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                            test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                            best_cv_score = score\n",
    "                            print(R, gamma, C, score, test_score)\n",
    "                            print(R, gamma, C, score, test_score, file=f)\n",
    "\n",
    "            return best_cv_score, test_score\n",
    "\n",
    "        print(train_and_predict(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "21dfdfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (1, 6)\n",
      "0 0.4 0.0001220703125 0.22778087377579329 0.20181111768635496\n",
      "5 0.4 0.0001220703125 0.20254744526977436 0.14195624644832938\n",
      "5 0.4 0.000244140625 0.16831936209519216 0.12606384178245242\n",
      "5 0.4 0.00048828125 0.14759287159289125 0.10979723957682785\n",
      "5 0.4 0.0009765625 0.13853449163996046 0.09425642869705556\n",
      "5 0.4 0.001953125 0.11982115476677713 0.08770085998869379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-56311db68e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_cv_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_coef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-148-56311db68e19>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mC\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mML_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXfeature_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neg_root_mean_squared_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mbest_cv_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mML_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXfeature_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[1;32m    446\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    248\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[1;32m    249\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[0;32m--> 250\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    251\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    252\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mthis_Xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_dual_gap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_iter\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m                 self.path(X, y[:, k],\n\u001b[0m\u001b[1;32m    845\u001b[0m                           \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                           \u001b[0mn_alphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 tol, rng, random, positive)\n\u001b[1;32m    529\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mprecompute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             model = cd_fast.enet_coordinate_descent(\n\u001b[0m\u001b[1;32m    531\u001b[0m                 \u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 positive)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm\n",
    "# Configured for outputting coefficients of linear model\n",
    "\n",
    "# set size of local region\n",
    "delta1 = 0\n",
    "\n",
    "# set max number of feature entries\n",
    "max_R = 1000\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "# set test size\n",
    "test_size = 0.4\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=test_size, random_state=0)\n",
    "\n",
    "# generate omega to pass into feature mapping\n",
    "omega = []\n",
    "for (q1, q2) in all_edges: # TODO: change this as well when changing feature mapping\n",
    "    m_local = len(get_local_region_edges(q1, q2, delta1))\n",
    "    omega_sub = []\n",
    "    for j in range(max_R):\n",
    "        omega_sub.append(np.random.normal(0, 1, m_local))\n",
    "    omega.append(omega_sub)\n",
    "\n",
    "with open('./results/{}_data_clf_coefficients/coefficient_{}x{}_delta1={}_local_feature_qubits{}.txt'.format(data_name, length, width, delta1, d), 'w') as f:\n",
    "    for (q1, q2) in qubits:\n",
    "        print('(q1, q2) =', (q1, q2))\n",
    "        print('(q1, q2) =', (q1, q2), file=f)\n",
    "\n",
    "        def train_and_predict():\n",
    "            # consider the pair (q1, q2)\n",
    "            global q1, q2\n",
    "\n",
    "            # training data (estimated from measurement data)\n",
    "            y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xfull_norm, y, test_size=test_size, random_state=0)\n",
    "\n",
    "            # testing data (exact expectation values)\n",
    "            y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "            _, _, _, y_test_clean = train_test_split(Xfull_norm, y_clean, test_size=test_size, random_state=0)\n",
    "\n",
    "            # use cross validation to find the best hyperparameters\n",
    "            best_cv_score, test_score = 999.0, 999.0\n",
    "            best_coef = 0\n",
    "            ML_method = lambda Cx : linear_model.Lasso(alpha=Cx, max_iter=30000)\n",
    "            # ML_method = lambda Cx: KernelRidge(kernel='linear', alpha=Cx)\n",
    "\n",
    "            for R in [0, 5, 10, 20, 40, 80, 160, 320]:\n",
    "                for gamma in [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 1.0]:\n",
    "                    # feature mapping\n",
    "                    Xfeature_train = get_feature_vectors(delta1, R, X_train, omega, gamma, q1, q2)\n",
    "                    Xfeature_test = get_feature_vectors(delta1, R, X_test, omega, gamma, q1, q2)\n",
    "\n",
    "                    for C in [2**(-13), 2**(-12), 2**(-11), 2**(-10), 2**(-9)]:\n",
    "                        score = -np.mean(cross_val_score(ML_method(C), Xfeature_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                        if best_cv_score > score:\n",
    "                            clf = ML_method(C).fit(Xfeature_train, y_train.ravel())\n",
    "                            test_score = np.linalg.norm(clf.predict(Xfeature_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                            best_cv_score = score\n",
    "                            best_coef = clf.coef_\n",
    "    #                             print(clf.coef_)\n",
    "                            print(R, gamma, C, score, test_score)\n",
    "\n",
    "            return best_cv_score, test_score, best_coef\n",
    "\n",
    "        print(train_and_predict()[2], file=f)\n",
    "        print(train_and_predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3b8b3d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructed Dirichlet kernel\n",
      "constructed neural tangent kernel\n",
      "RBF kernel (will be constructed in sklearn)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Dirichlet kernel\n",
    "#\n",
    "\n",
    "kernel_dir = np.zeros((len(Xfull), Xfull.shape[1]*5))\n",
    "for i, x1 in enumerate(Xfull):\n",
    "    cnt = 0\n",
    "    for k in range(len(x1)):\n",
    "        for k1 in range(-2, 3):\n",
    "            kernel_dir[i, cnt] += np.cos(np.pi * k1 * x1[k])\n",
    "            cnt += 1\n",
    "print(\"constructed Dirichlet kernel\")\n",
    "            \n",
    "#\n",
    "# Neural tangent kernel\n",
    "#\n",
    "    \n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(1)\n",
    ")\n",
    "kernel_NN2 = kernel_fn(Xfull, Xfull, 'ntk')\n",
    "\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(1)\n",
    ")\n",
    "kernel_NN3 = kernel_fn(Xfull, Xfull, 'ntk')\n",
    "                \n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(1)\n",
    ")\n",
    "kernel_NN4 = kernel_fn(Xfull, Xfull, 'ntk')\n",
    "\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(32), stax.Relu(),\n",
    "    stax.Dense(1)\n",
    ")\n",
    "kernel_NN5 = kernel_fn(Xfull, Xfull, 'ntk')\n",
    "\n",
    "list_kernel_NN = [kernel_NN2, kernel_NN3, kernel_NN4, kernel_NN5]\n",
    "\n",
    "for r in range(len(list_kernel_NN)):\n",
    "    kernel = list_kernel_NN[r].copy()\n",
    "    for i in range(len(list_kernel_NN[r])):\n",
    "        for j in range(len(list_kernel_NN[r])):\n",
    "            # list_kernel_NN[r][i][j] /= (list_kernel_NN[r][i][i] * list_kernel_NN[r][j][j]) ** 0.5\n",
    "            list_kernel_NN[r][i].at[j].divide((kernel[i][i] * kernel[j][j]) ** 0.5)\n",
    "print(\"constructed neural tangent kernel\")\n",
    "            \n",
    "#\n",
    "# RBF kernel is defined in Sklearn\n",
    "#\n",
    "print(\"RBF kernel (will be constructed in sklearn)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "11d497a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(q1, q2) = (4, 5)\n",
      "Dirich. kernel (0.2524725471445278, 0.24720161876210964)\n",
      "Gaussi. kernel (0.18580648811215386, 0.19334888028240627)\n",
      "Neur. T kernel (0.13558781374889525, 0.1640358656018234)\n",
      "Neur. T kernel (0.1397908436522027, 0.16704989523840308)\n",
      "Neur. T kernel (0.16193101833276144, 0.18731811705888335)\n",
      "Neur. T kernel (0.19950551325783114, 0.21130932478677994)\n"
     ]
    }
   ],
   "source": [
    "# Training and testing algorithm (Old method)\n",
    "\n",
    "# set of pairs of qubits we care about predicting correlation function for\n",
    "d = 1\n",
    "qubits = get_nearby_qubit_pairs(d)\n",
    "\n",
    "test_size = 0.4\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(range(len(Xfull)), range(len(Xfull)), test_size=test_size, random_state=0)\n",
    "\n",
    "#with open('./results/orig_algorithm/orig_algorithm_test_size={}/orig_algorithm_{}_data_k/results_{}x{}_all_qubits.txt'.format(test_size, data_name, length, width), 'w') as f:\n",
    "for (q1, q2) in qubits[7:8]:\n",
    "    # each k corresponds to the correlation function in a pair of qubits\n",
    "    print(\"(q1, q2) = ({}, {})\".format(q1, q2))\n",
    "    #print(\"k =\", k, file=f)\n",
    "\n",
    "    def train_and_predict(kernel, opt=\"linear\"): # opt=\"linear\" or \"rbf\"\n",
    "\n",
    "        # instance-wise normalization\n",
    "        for i in range(len(kernel)):\n",
    "            if type(kernel) == np.ndarray:\n",
    "                kernel[i] /= np.linalg.norm(kernel[i])\n",
    "            else:\n",
    "                kernel.at[i].divide(np.linalg.norm(kernel[i]))\n",
    "\n",
    "        # consider the k-th pair\n",
    "        global q1, q2\n",
    "\n",
    "        # training data (estimated from measurement data)\n",
    "        y = np.array([Ytrain[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(kernel, y, test_size=test_size, random_state=0)\n",
    "\n",
    "        # testing data (exact expectation values)\n",
    "        y_clean = np.array([Yfull[i].reshape((length * width, length * width))[q1 - 1][q2 - 1] for i in range(len(Xfull))])\n",
    "        _, _, _, y_test_clean = train_test_split(kernel, y_clean, test_size=test_size, random_state=0)\n",
    "\n",
    "        # use cross validation to find the best method + hyper-param\n",
    "        best_cv_score, test_score = 999.0, 999.0\n",
    "        for ML_method in [(lambda Cx: svm.SVR(kernel=opt, C=Cx)), (lambda Cx: KernelRidge(kernel=opt, alpha=1/(2*Cx)))]:\n",
    "            for C in [0.0125, 0.025, 0.05, 0.125, 0.25, 0.5, 1.0, 2.0]:\n",
    "                score = -np.mean(cross_val_score(ML_method(C), X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"))\n",
    "                if best_cv_score > score:\n",
    "                    clf = ML_method(C).fit(X_train, y_train.ravel())\n",
    "                    test_score = np.linalg.norm(clf.predict(X_test).ravel() - y_test_clean.ravel()) / (len(y_test) ** 0.5)\n",
    "                    best_cv_score = score\n",
    "\n",
    "        return best_cv_score, test_score\n",
    "\n",
    "    # Dirichlet\n",
    "    #print(\"Dirich. kernel\", train_and_predict(kernel_dir), file=f)\n",
    "    print(\"Dirich. kernel\", train_and_predict(kernel_dir))\n",
    "    # RBF\n",
    "    #print(\"Gaussi. kernel\", train_and_predict(Xfull, opt=\"rbf\"), file=f)\n",
    "    print(\"Gaussi. kernel\", train_and_predict(Xfull, opt=\"rbf\"))\n",
    "    # Neural tangent\n",
    "    for kernel_NN in list_kernel_NN:\n",
    "        #print(\"Neur. T kernel\", train_and_predict(kernel_NN), file=f)\n",
    "        print(\"Neur. T kernel\", train_and_predict(kernel_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790a8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
